{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/703308922/Documents/latent-diffusion/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/703308922/Documents/latent-diffusion/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPModel, CLIPProcessor\n",
    "from huggingface_hub import login\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Set device to MPS if available, else CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 20.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load the pipeline\n",
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\"\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = pipeline.vae\n",
    "unet = pipeline.unet\n",
    "text_encoder = pipeline.text_encoder\n",
    "tokenizer = pipeline.tokenizer\n",
    "scheduler = pipeline.scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CLIP model (both image and text encoders)\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your data (update these paths)\n",
    "images_dir = \"./flickr30k_images/flickr30k_images\"  # Update this path\n",
    "captions_file = \"./flickr30k_images/results.csv\"    # Update this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom Dataset class\n",
    "class Flickr30KDataset(Dataset):\n",
    "    def __init__(self, images_dir, captions_file, tokenizer, clip_tokenizer, clip_transform, sd_transform):\n",
    "        self.images_dir = images_dir\n",
    "        self.captions_file = captions_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.clip_tokenizer = clip_tokenizer\n",
    "        self.clip_transform = clip_transform\n",
    "        self.sd_transform = sd_transform\n",
    "\n",
    "        # Load captions from the CSV file\n",
    "        self.captions_df = pd.read_csv(\n",
    "            self.captions_file,\n",
    "            sep=\"|\",\n",
    "            header=0,\n",
    "            encoding='utf-8',\n",
    "            on_bad_lines='skip'  # For pandas >=1.3.0\n",
    "        )\n",
    "\n",
    "        # Ensure the image names are strings\n",
    "        self.captions_df['image_name'] = self.captions_df['image_name'].astype(str)\n",
    "\n",
    "        # Group captions by image_name\n",
    "        self.image_captions = self.captions_df.groupby('image_name')['comment'].apply(list).to_dict()\n",
    "        self.image_names = list(self.image_captions.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image name\n",
    "        image_name = self.image_names[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Apply CLIP transform to image\n",
    "        image_clip = self.clip_transform(image)\n",
    "        # Apply Stable Diffusion transform to image\n",
    "        image_sd = self.sd_transform(image)\n",
    "\n",
    "        # Get captions for the image\n",
    "        captions = self.image_captions[image_name]\n",
    "        # For simplicity, we'll use the first caption\n",
    "        caption = captions[0]\n",
    "\n",
    "        # Tokenize the caption using the Stable Diffusion tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize the caption using the CLIP tokenizer (for the CLIP text encoder)\n",
    "        clip_encoding = self.clip_tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=77,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image_clip\": image_clip,  # Tensor\n",
    "            \"image_sd\": image_sd,      # Tensor\n",
    "            \"clip_input_ids\": clip_encoding[\"input_ids\"].squeeze(),\n",
    "            \"clip_attention_mask\": clip_encoding[\"attention_mask\"].squeeze(),\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations\n",
    "# For the CLIP image encoder (requires 224x224 images)\n",
    "clip_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "                         std=(0.26862954, 0.26130258, 0.27577711)),\n",
    "])\n",
    "\n",
    "# For the Stable Diffusion pipeline (expects 512x512 images)\n",
    "sd_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer (from the pipeline)\n",
    "sd_tokenizer = tokenizer\n",
    "\n",
    "# CLIP tokenizer\n",
    "clip_tokenizer = clip_processor.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the dataset\n",
    "dataset = Flickr30KDataset(\n",
    "    images_dir=images_dir,\n",
    "    captions_file=captions_file,\n",
    "    tokenizer=sd_tokenizer,\n",
    "    clip_tokenizer=clip_tokenizer,\n",
    "    clip_transform=clip_transform,\n",
    "    sd_transform=sd_transform\n",
    ")\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 1  # Adjust based on your hardware capabilities\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Fine-tuning the text_encoder\n",
    "text_encoder.train()\n",
    "clip_model.eval()  # We'll use the CLIP image encoder in evaluation mode\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(text_encoder.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Stable Diffusion pipeline (expects 512x512 images)\n",
    "sd_transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFlickr30KDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcaptions_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcaptions_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# We'll apply transforms separately\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Create the DataLoader\u001b[39;00m\n\u001b[1;32m     10\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Adjust based on your hardware capabilities\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "dataset = Flickr30KDataset(\n",
    "    images_dir=images_dir,\n",
    "    captions_file=captions_file,\n",
    "    tokenizer=tokenizer,\n",
    "    transform=None  # We'll apply transforms separately\n",
    ")\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 1  # Adjust based on your hardware capabilities\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Fine-tuning the text_encoder\n",
    "text_encoder.train()\n",
    "clip_model.eval()  # We'll use the CLIP image encoder in evaluation mode\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(text_encoder.parameters(), lr=5e-5)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fine-tuning Text Encoder Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch in tqdm(dataloader, desc=f\"Fine-tuning Text Encoder Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        images_clip = batch[\"image_clip\"].to(device)  # Already a tensor\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        clip_input_ids = batch[\"clip_input_ids\"].to(device)\n",
    "        clip_attention_mask = batch[\"clip_attention_mask\"].to(device)\n",
    "        \n",
    "        # Get image embeddings using CLIP image encoder\n",
    "        with torch.no_grad():\n",
    "            image_embeddings = clip_model.get_image_features(images_clip)\n",
    "        \n",
    "        # Normalize image embeddings\n",
    "        image_embeddings = image_embeddings / image_embeddings.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Get text embeddings from the text_encoder (Stable Diffusion's text encoder)\n",
    "        text_outputs = text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        text_embeddings = text_outputs.last_hidden_state\n",
    "        # Take the mean pooling over the sequence dimension\n",
    "        text_embeddings = text_embeddings.mean(dim=1)\n",
    "        # Normalize text embeddings\n",
    "        text_embeddings = text_embeddings / text_embeddings.norm(p=2, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Compute similarity scores\n",
    "        logits_per_image = image_embeddings @ text_embeddings.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "        \n",
    "        # Labels\n",
    "        batch_size = images_clip.size(0)\n",
    "        labels = torch.arange(batch_size).to(device)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss_image = criterion(logits_per_image, labels)\n",
    "        loss_text = criterion(logits_per_text, labels)\n",
    "        loss = (loss_image + loss_text) / 2\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:50<00:00,  1.01s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned text encoder\n",
    "text_encoder.save_pretrained(\"fine_tuned_text_encoder\")\n",
    "\n",
    "# Update the pipeline's text encoder\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"fine_tuned_text_encoder\").to(device)\n",
    "pipeline.text_encoder = text_encoder\n",
    "\n",
    "# Proceed to generate images using the fine-tuned pipeline\n",
    "pipeline.to(device)\n",
    "pipeline.enable_attention_slicing()\n",
    "\n",
    "# Generate an image\n",
    "prompt = \"A beautiful landscape with mountains and a lake.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = pipeline(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "\n",
    "# Display the image\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
